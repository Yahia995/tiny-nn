<div align="center">

# ðŸ§  Tiny Neural Network in C++

![C++](https://img.shields.io/badge/C++-20-00599C?style=for-the-badge&logo=cplusplus&logoColor=white)
![CMake](https://img.shields.io/badge/CMake-3.20+-064F8C?style=for-the-badge&logo=cmake&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)
![Build](https://img.shields.io/badge/Build-Passing-brightgreen?style=for-the-badge)
![No Dependencies](https://img.shields.io/badge/Dependencies-None-orange?style=for-the-badge)
![Platform](https://img.shields.io/badge/Platform-Linux%20|%20macOS%20|%20Windows-lightgrey?style=for-the-badge)

**A minimal neural network engine implemented from scratch in modern C++20 â€” no external ML libraries.**

</div>

---

## âœ¨ Features

- **Matrix Engine** â€” Dynamic, row-major matrix class with full operator support
- **Matrix Ops** â€” Multiplication, transpose, scalar operations
- **Activations** â€” ReLU, Sigmoid (scalar & element-wise matrix versions)
- **Loss Function** â€” Binary Cross Entropy with gradient
- **Dense Layer** â€” Fully connected layer with configurable activation
- **Backpropagation** â€” Gradient descent with chain rule
- **XOR Example** â€” Classic non-linear classification problem

---

## ðŸ—ï¸ Architecture

### Network Topology

```mermaid
graph TD
    I1((xâ‚)) --> H1
    I1 --> H2
    I1 --> H3
    I1 --> H4
    I1 --> H5
    I1 --> H6
    I1 --> H7
    I1 --> H8
    I2((xâ‚‚)) --> H1
    I2 --> H2
    I2 --> H3
    I2 --> H4
    I2 --> H5
    I2 --> H6
    I2 --> H7
    I2 --> H8
    H1((hâ‚)) --> O((Å·))
    H2((hâ‚‚)) --> O
    H3((hâ‚ƒ)) --> O
    H4((hâ‚„)) --> O
    H5((hâ‚…)) --> O
    H6((hâ‚†)) --> O
    H7((hâ‚‡)) --> O
    H8((hâ‚ˆ)) --> O

    subgraph Input["Input Layer (2)"]
        I1
        I2
    end

    subgraph Hidden["Hidden Layer (8) Â· ReLU"]
        H1
        H2
        H3
        H4
        H5
        H6
        H7
        H8
    end

    subgraph Output["Output Layer (1) Â· Sigmoid"]
        O
    end

    style Input fill:#1e3a5f,color:#fff,stroke:#4a90d9
    style Hidden fill:#1a4731,color:#fff,stroke:#4caf7d
    style Output fill:#4a1942,color:#fff,stroke:#c06dd9
```

### Forward & Backward Pass

```mermaid
sequenceDiagram
    participant IN as Input
    participant L1 as Dense Layer 1<br/>(ReLU)
    participant L2 as Dense Layer 2<br/>(Sigmoid)
    participant LF as Loss (BCE)

    Note over IN,LF: â”€â”€ Forward Pass â”€â”€
    IN->>L1: x (2Ã—1)
    L1->>L2: h = ReLU(Wâ‚x + bâ‚) (8Ã—1)
    L2->>LF: Å· = Ïƒ(Wâ‚‚h + bâ‚‚) (1Ã—1)
    LF-->>LF: L = -[yÂ·log(Å·) + (1-y)Â·log(1-Å·)]

    Note over IN,LF: â”€â”€ Backward Pass â”€â”€
    LF->>L2: âˆ‚L/âˆ‚Å·
    L2->>L1: âˆ‚L/âˆ‚h (update Wâ‚‚, bâ‚‚)
    L1->>IN: âˆ‚L/âˆ‚x (update Wâ‚, bâ‚)
```

---

## ðŸ“ Project Structure

```mermaid
graph LR
    ROOT[tiny_nn/] --> SRC[src/]
    ROOT --> BUILD[build/]
    ROOT --> CMAKE[CMakeLists.txt]
    ROOT --> README[README.md]
    ROOT --> LICENSE[LICENSE]

    SRC --> MAIN[main.cpp]
    SRC --> MAT_H[matrix.hpp]
    SRC --> MAT_C[matrix.cpp]
    SRC --> LAYER_H[layer.hpp]
    SRC --> LAYER_C[layer.cpp]
    SRC --> ACT[activations.hpp]
    SRC --> LOSS[loss.hpp]

    style ROOT fill:#2d2d2d,color:#fff,stroke:#888
    style SRC fill:#1e3a5f,color:#fff,stroke:#4a90d9
    style BUILD fill:#3a3a1e,color:#fff,stroke:#d9c74a
```

---

## ðŸš€ Build & Run

```bash
mkdir build
cd build
cmake ..
make
./tiny_nn
```

---

## ðŸ“Š Training Flow

```mermaid
flowchart TD
    A([Start]) --> B[Initialize Layers\nXavier Weights]
    B --> C{epoch < 10000?}
    C -- Yes --> D[Sample from XOR Dataset]
    D --> E[Forward Pass\nLayer 1 ReLU]
    E --> F[Forward Pass\nLayer 2 Sigmoid]
    F --> G[Compute BCE Loss]
    G --> H[Compute âˆ‚L/âˆ‚Å·]
    H --> I[Backward Layer 2\nUpdate Wâ‚‚, bâ‚‚]
    I --> J[Backward Layer 1\nUpdate Wâ‚, bâ‚]
    J --> K{All samples done?}
    K -- No --> D
    K -- Yes --> L{epoch % 1000 == 0?}
    L -- Yes --> M[Print Avg Loss]
    L -- No --> C
    M --> C
    C -- No --> N[Run Final Predictions]
    N --> O([End])

    style A fill:#1a4731,color:#fff,stroke:none
    style O fill:#4a1942,color:#fff,stroke:none
    style G fill:#1e3a5f,color:#fff,stroke:#4a90d9
    style M fill:#3a2a0a,color:#fff,stroke:#d9a84a
```

---

## ðŸ“ˆ Example Output

```
Epoch 0    Loss: 0.7231
Epoch 1000 Loss: 0.6891
Epoch 2000 Loss: 0.5204
Epoch 3000 Loss: 0.3012
...
Epoch 9000 Loss: 0.0183

Final predictions:
[0,0] â†’ 0.02  âœ“
[0,1] â†’ 0.97  âœ“
[1,0] â†’ 0.97  âœ“
[1,1] â†’ 0.03  âœ“
```

---

## ðŸ”­ Future Improvements

| Feature | Status |
|---|---|
| Batch training | ðŸ”² Planned |
| Softmax + multi-class | ðŸ”² Planned |
| MNIST dataset | ðŸ”² Planned |
| SIMD optimization | ðŸ”² Planned |
| OpenMP parallelization | ðŸ”² Planned |
| CUDA implementation | ðŸ”² Planned |

---

<div align="center">

**Author:** [@Yahia995](https://github.com/Yahia995) &nbsp;Â·&nbsp; **License:** MIT

</div>
